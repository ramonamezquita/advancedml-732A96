---
title: 'LAB 3: REINFORCEMENT LEARNING'
author:
  - Linh Nguyen^[linng299@student.liu.se]
  - Ramón Amézquita Vizcarra^[ramam120@student.liu.se]
  - Sofia Danielson^[sofda069@student.liu.se]
  - Thor Wahlestedt^[thowa194@student.liu.se]
date: "`r Sys.Date()`"
output:
  pdf_document:
    latex_engine: pdflatex
    fig_caption: yes
    number_sections: true
  html_document: default
    
header-includes:
 \usepackage{float}
 \setlength{\textfloatsep}{0pt}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H',fig.height = 4, fig.align = 'center')
```

```{r include=FALSE}
# By Jose M. Peña and Joel Oskarsson.
# For teaching purposes.
# jose.m.pena@liu.se.

#####################################################################################################
# Q-learning
#####################################################################################################

# install.packages("ggplot2")
# install.packages("vctrs")
library(ggplot2)

arrows <- c("^", ">", "v", "<")
action_deltas <- list(c(1,0), # up
                      c(0,1), # right
                      c(-1,0), # down
                      c(0,-1)) # left

vis_environment <- function(iterations=0, epsilon = 0.5, alpha = 0.1, gamma = 0.95, beta = 0){
  
  # Visualize an environment with rewards. 
  # Q-values for all actions are displayed on the edges of each tile.
  # The (greedy) policy for each state is also displayed.
  # 
  # Args:
  #   iterations, epsilon, alpha, gamma, beta (optional): for the figure title.
  #   reward_map (global variable): a HxW array containing the reward given at each state.
  #   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  #   H, W (global variables): environment dimensions.
  
  df <- expand.grid(x=1:H,y=1:W)
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,1],NA),df$x,df$y)
  df$val1 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,2],NA),df$x,df$y)
  df$val2 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,3],NA),df$x,df$y)
  df$val3 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,q_table[x,y,4],NA),df$x,df$y)
  df$val4 <- as.vector(round(foo, 2))
  foo <- mapply(function(x,y) 
    ifelse(reward_map[x,y] == 0,arrows[GreedyPolicy(x,y)],reward_map[x,y]),df$x,df$y)
  df$val5 <- as.vector(foo)
  foo <- mapply(function(x,y) ifelse(reward_map[x,y] == 0,max(q_table[x,y,]),
                                     ifelse(reward_map[x,y]<0,NA,reward_map[x,y])),df$x,df$y)
  df$val6 <- as.vector(foo)
  
  print(ggplot(df,aes(x = y,y = x)) +
          scale_fill_gradient(low = "white", high = "green", na.value = "red", name = "") +
          geom_tile(aes(fill=val6)) +
          geom_text(aes(label = val1),size = 3,nudge_y = .35,na.rm = TRUE) +
          geom_text(aes(label = val2),size = 3,nudge_x = .35,na.rm = TRUE) +
          geom_text(aes(label = val3),size = 3,nudge_y = -.35,na.rm = TRUE) +
          geom_text(aes(label = val4),size = 3,nudge_x = -.35,na.rm = TRUE) +
          geom_text(aes(label = val5),size = 10) +
          geom_tile(fill = 'transparent', colour = 'black') + 
          ggtitle(paste("Q-table after ",iterations," iterations\n",
                        "(epsilon = ",epsilon,", alpha = ",alpha,"gamma = ",gamma,", beta = ",beta,")")) +
          theme(plot.title = element_text(hjust = 0.5)) +
          scale_x_continuous(breaks = c(1:W),labels = c(1:W)) +
          scale_y_continuous(breaks = c(1:H),labels = c(1:H)))
  
}

GreedyPolicy <- function(x, y){
  
  # Get a greedy action for state (x,y) from q_table.
  #
  # Args:
  #   x, y: state coordinates.
  #   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  # 
  # Returns:
  #   An action, i.e. integer in {1,2,3,4}.
  
  #Your code here.
  max.action = which(q_table[x, y, ] == max(q_table[x, y, ]))
  if (length(max.action) > 1) return(sample(max.action, 1))
  else return(max.action)
  
}

EpsilonGreedyPolicy <- function(x, y, epsilon){
  
  # Get an epsilon-greedy action for state (x,y) from q_table.
  #
  # Args:
  #   x, y: state coordinates.
  #   epsilon: probability of acting randomly.
  # 
  # Returns:
  #   An action, i.e. integer in {1,2,3,4}.
  
  # Your code here.
  probs <- rep(epsilon/4, 4)
  q_values <- q_table[x,y,]
  
  #find index of maximal Q-values
  max_index <- which(q_values == max(q_values))
  #add the extra 1-epsilon amongs the maximum q-value holders
  probs[max_index] <- probs[max_index] + (1-epsilon)/length(max_index)
  #sample the action and return
  action <- sample(1:4, size=1, prob = probs)
  return(action)
}

transition_model <- function(x, y, action, beta){
  
  # Computes the new state after given action is taken. The agent will follow the action 
  # with probability (1-beta) and slip to the right or left with probability beta/2 each.
  # 
  # Args:
  #   x, y: state coordinates.
  #   action: which action the agent takes (in {1,2,3,4}).
  #   beta: probability of the agent slipping to the side when trying to move.
  #   H, W (global variables): environment dimensions.
  # 
  # Returns:
  #   The new state after the action has been taken.
  
  delta <- sample(-1:1, size = 1, prob = c(0.5*beta,1-beta,0.5*beta))
  final_action <- ((action + delta + 3) %% 4) + 1
  foo <- c(x,y) + unlist(action_deltas[final_action])
  foo <- pmax(c(1,1),pmin(foo,c(H,W)))
  
  return (foo)
}

q_learning <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95, 
                       beta = 0){
  
  # Perform one episode of Q-learning. The agent should move around in the 
  # environment using the given transition model and update the Q-table.
  # The episode ends when the agent reaches a terminal state.
  # 
  # Args:
  #   start_state: array with two entries, describing the starting position of the agent.
  #   epsilon (optional): probability of acting randomly.
  #   alpha (optional): learning rate.
  #   gamma (optional): discount factor.
  #   beta (optional): slipping factor.
  #   reward_map (global variable): a HxW array containing the reward given at each state.
  #   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  # 
  # Returns:
  #   reward: reward received in the episode.
  #   correction: sum of the temporal difference correction terms over the episode.
  #   q_table (global variable): Recall that R passes arguments by value. So, q_table being
  #   a global variable can be modified with the superassigment operator <<-.
  
  # Your code here.
  episode_correction <- 0
  state <- start_state
  repeat{
    #we first get the desired action based on greedy epsilon 
    desired_act <- EpsilonGreedyPolicy(state[1], state[2], epsilon)
    #we (at least attempt) to execute the desired act and see where we land and
    #what reward we get
    new_state <- transition_model(state[1], state[2], desired_act, beta)
    reward <- reward_map[new_state[1], new_state[2]]
    
    score_target <- reward+gamma*max(q_table[new_state[1], new_state[2],])
    curr_q_score <- q_table[state[1], state[2], desired_act]
    diff <- score_target - curr_q_score

    #updatethe global q_table 
    q_table[state[1], state[2], desired_act] <<- curr_q_score+alpha*diff

    #accumulate corrections
    episode_correction <- episode_correction+abs(diff)
    
    
    state <- new_state
    # Follow policy, execute action, get reward.
    
    # Q-table update.
    
    if(reward!=0)
      # End episode.
      return (c(reward,episode_correction))
  }
  
}

```

# Q-Learning {#ql}

This part of the lab focuses on the concepts of Q-learning covered in the lectures. The file `RL_Lab1.R` in the course website contains a template of the Q-learning algorithm. We let our agent interact with three different environments using distinct policies. In particular, we will be working on a 2D discrete environment of dimension $H \times W$ where the agent is allowed to move up, down, left, or right. This corresponds to the following Markov decision process:

\begin{itemize}
  \item State Space: $\mathcal{S} = \left\{(x,y) \mid x \in \left\{1, \dots, H \right\}, y \in \left\{1, \dots, W \right\}  \right\}$
  \item Action space: $\mathcal{A} = \left\{\text{up, down, left, right}\right\}$
\end{itemize}

We are asked to complete the implementation of the algorithm through the given tasks and then answer some questions. First things first, we need to complete the implementation. The first task is to implement the greedy and $\epsilon$-greedy policies in the functions `GreedyPolicy` and `EpsilonGreedyPolicy`. The second task is to implement the function `q-learning`. With both tasks completed, we can continue with the rest of the Q-learning lab.

## Environment A {#envA}

For our the first environment, we will use $H = 5$ and $W = 7$. This environment includes a reward of 10 in state (3,6). We are tasked to answer questions after running 10000 episodes of Q-learning in this environment.

```{r echo=FALSE}

#####################################################################################################
# Q-Learning Environments
#####################################################################################################

# Environment A (learning)

H <- 5
W <- 7

reward_map <- matrix(0, nrow = H, ncol = W)
reward_map[3,6] <- 10
reward_map[2:4,3] <- -1

q_table <- array(0,dim = c(H,W,4))

vis_environment()

for(i in 1:10000){
  foo <- q_learning(start_state = c(3,1))
  
  if(any(i==c(10,100,1000,10000)))
    vis_environment(i)
}

```

**What has the agent learned after the first 10 episodes?**

After 10 iterations, it does not seem like the sequential learning model has gained a lot of meaningful information about the Q-values, with the exception of some Q-values in tiles next to the negative terminal states, which imply the negative reward of moving on to the negative terminal states. We clearly need to do more iterations in order to explore more states and actions from those states in order to learn how to move towards the state with +10 reward.

**Is the final greedy policy (after 10000 episodes) optimal for all states, i.e. not only for the initial state? Why/Why not?**

It depends on what is necessarily meant by optimal. In the sense of reward it does seem like all possible tiles will eventually end up in the positive terminal state and clearly avoid the negative terminal states, which means that we will always get the same "optimal" total reward. But If we meant optimal by the shortest in terms of tiles stepped on then no. If we want the agent to prioritize the shortest path, we might decide to add a small negative reward on every non terminal.

**Do the learned values in the Q-table reflect the fact that there are multiple paths?**

The q-values for the up and down actions on the states left to the negative terminal states does seem to be fairly close to each other. This indicates the potential to both go up and down when walking around the negative terminal states if using a soft approach of choosing the next action according to policy (for example epsilon greedy policy) 

## Environment B {#envB}

We now work in an environment with two positive rewards, of 5 and 10. The goal is for the agent to navigate around the reward of 5 in order to reach the reward worth 10. The task is to investigate how the $\epsilon$ and $\gamma$ parameters affect the learned policy by running 30000 episodes of Q-learning with $\epsilon = 0.1, 0.5$ and $\gamma = 0.5, 0.75, 0.95$.

```{r echo=FALSE}
H <- 7
W <- 8

reward_map <- matrix(0, nrow = H, ncol = W)
reward_map[1,] <- -1
reward_map[7,] <- -1
reward_map[4,5] <- 5
reward_map[4,8] <- 10

q_table <- array(0,dim = c(H,W,4))

vis_environment()

MovingAverage <- function(x, n){
  
  cx <- c(0,cumsum(x))
  rsum <- (cx[(n+1):length(cx)] - cx[1:(length(cx) - n)]) / n
  
  return (rsum)
}

for(j in c(0.5,0.75,0.95)){
  q_table <- array(0,dim = c(H,W,4))
  reward <- NULL
  correction <- NULL
  
  for(i in 1:30000){
    foo <- q_learning(gamma = j, start_state = c(4,1))
    reward <- c(reward,foo[1])
    correction <- c(correction,foo[2])
  }
  
  vis_environment(i, gamma = j)
  plot(MovingAverage(reward,100),type = "l")
  plot(MovingAverage(correction,100),type = "l")
}

for(j in c(0.5,0.75,0.95)){
  q_table <- array(0,dim = c(H,W,4))
  reward <- NULL
  correction <- NULL
  
  for(i in 1:30000){
    #foo <- q_learning(epsilon = 0.1, gamma = j, start_state = c(4,1))
    foo <- q_learning(epsilon = 0.5, gamma = j, start_state = c(4,1))
    reward <- c(reward,foo[1])
    correction <- c(correction,foo[2])
  }
  
  #vis_environment(i, epsilon = 0.1, gamma = j)
  vis_environment(i, epsilon = 0.5, gamma = j)
  plot(MovingAverage(reward,100),type = "l")
  plot(MovingAverage(correction,100),type = "l")
}
```

**Effect of** $\boldsymbol{\gamma}$. From the plots, we can surmise that higher values of $\gamma$ allow the model to prioritize greater future rewards than lesser immediate ones. At a $\gamma$ of 0.5, the agent seems to be content with the easier achieved reward of 5, whereas when $\gamma$ is turned up to 0.95, it is able to more consistently circumvent the reward of 5 in order to achieve the reward of 10, despite the longer path.

**Effect of** $\boldsymbol{\epsilon}$. Similarly, the agent is able to more consistently locate and achieve the reward of 10 with a higher value for $\epsilon$. When $\epsilon$ is kept at 0.1, the agent seems satisfied with the shorter term, more achievable reward of 5. Although a higher value for $\epsilon$ (and similarly a higher value for $\gamma$) might cause learning to take longer to plateau, it also ensures a more favorable performance once that plateau is achieved.

## Environment C {#envC}

This is a smaller environment, where the agent starts each episode in the state (1,1). The task is to investigate how the $\beta$ parameter affects the learned policy by running 10000 episodes of Q-learning with $\beta = 0, 0.2, 0.4, 0.66$.

```{r echo=FALSE}
# Environment C (the effect of beta).

H <- 3
W <- 6

reward_map <- matrix(0, nrow = H, ncol = W)
reward_map[1,2:5] <- -1
reward_map[1,6] <- 10

q_table <- array(0,dim = c(H,W,4))

vis_environment()

for(j in c(0,0.2,0.4,0.66)){
  q_table <- array(0,dim = c(H,W,4))
  
  for(i in 1:10000)
    foo <- q_learning(gamma = 0.6, beta = j, start_state = c(1,1))
  
  vis_environment(i, gamma = 0.6, beta = j)
}
```

**Effect of** $\boldsymbol{\beta}$. When $\beta$ equals 0, the model seems pretty deterministic (greedy policy) which means it should converge relatively quickly. As we increase $\beta$, we notice the Q-values get less extreme and the agent prioritizes "safety" more. However, at a $\beta$ of 0.66 the model starts to act so conservatively that it begins to negatively impact performance.



# REINFORCE {#Re}

We consider a $4\times4$ grid in which an agent learns to navigate from a random initial position
to a random goal position. On every step, the state of the agent is represented by its location and the goal locations. That is,

$$
s = \left(x_{agent}, y_{agent}, x_{goal}, y_{goal} \right)
$$
where $\left(x_{agent}, y_{agent}\right)$ is the location of the agent and $\left(x_{goal}, y_{goal} \right)$ indicates the goal location. Since the goal position changes randomly in each episode, the policy must depend on both the agent's and the goal's coordinates.

## Environment D {#envD}

To solve this task, we simply study the code and the provided results to answer given questions.

**Has the agent learned a good policy? Why / Why not ?**

The arrows across the grid consistently point toward the goal for all validation goal positions. This observation strongly indicates the agent learned a good goal-conditioned navigation policy and was able to generalize consistently. That also means that the agent learned the relationship between agent $\rightarrow$ goal rather than relying on memorizing specific (start, goal) pairs.

**Could you have used the Q-learning algorithm to solve this task ?**

Yes. The Q-table will grow in size though since we would need to add the location of the goal in the current state. The state space $\mathcal{S}$ and action set $\mathcal{A}$ thus becomes

\begin{itemize}
  \item State Space: $\mathcal{S} = \mathcal{S}{agent} \times \mathcal{S}{goal}$
  \item Action space: $\mathcal{A} = \left\{\text{up, down, left, right}\right\}$
\end{itemize}

where $\mathcal{S}{agent} = \mathcal{S}{goal}$ is the $4\times 4$ square grid. So, the Q-table size is: $16 \times 16 \times 4 = 1024$, which it is still very manageable.


## Environment E {#envE}

To solve this task, we simply study the code and the provided results to answer given questions.

**Has the agent learned a good policy? Why / Why not?**

We can conclude, by observing the visualized validated policy, that the agent has yet to learn an effective policy. We have observed that after 5000 episodes, not all states' actions, which have the highest probabilities, are on paths leading to the goal

**If the results obtained for environments D and E differ, explain why.**

Environment D uses eight goal positions for training and then validates the learned policy on the remaining eight possible goal positions. Meanwhile, environment E uses only three training goal positions, which belong to the top row of the grid, and then validates three positions from the rows below. This choice results in disproportion in learned actions, considering the agent has yet to learn to move downward, as all the training goals in E don't require the agent to move downward, since those goals belong to the top row. Thus, the agent can only approach the validating goals through leftward, rightward, or upward action.



# Contribution

For this lab, we have decided to divide into two pairs to work on Q-learning and REINFORCE, respectively. This decision was due to our conflicting schedules. Each member worked individually, and then addressed their pair group to reach coherent solutions to the assigned part. Each pair group represented their part in a group discussion, while we all added final comments and compared them to our own individual conclusions. This report was created with the solutions to environment [A](#envA) from Sofia, to environment [B](#envB) and [C](#envC) from Thor, to environment [D](#envD) from Ramón, and to environment [E](#envE) from Linh. The report finalization was by the entire group.

# Appendix

``` r

GreedyPolicy <- function(x, y){
  
  # Get a greedy action for state (x,y) from q_table.
  #
  # Args:
  #   x, y: state coordinates.
  #   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  # 
  # Returns:
  #   An action, i.e. integer in {1,2,3,4}.
  
  
  q_values <- q_table[x,y,]
  #find index of maximal Q-values
  imax <- which(q_values==max(q_values))
  #sample from the maximum indexes
  return(sample(imax, size=1))
  
}


EpsilonGreedyPolicy <- function(x, y, epsilon){
  
  # Get an epsilon-greedy action for state (x,y) from q_table.
  #
  # Args:
  #   x, y: state coordinates.
  #   epsilon: probability of acting randomly.
  # 
  # Returns:
  #   An action, i.e. integer in {1,2,3,4}.
  
  
  probs <- rep(epsilon/4, 4)
  q_values <- q_table[x,y,]
  
  #find index of maximal Q-values
  max_index <- which(q_values == max(q_values))
  #add the extra 1-epsilon amongs the maximum q-value holders
  probs[max_index] <- probs[max_index] + (1-epsilon)/length(max_index)
  #sample the action and return
  action <- sample(1:4, size=1, prob = probs)
  return(action)
}


q_learning <- function(start_state, epsilon = 0.5, alpha = 0.1, gamma = 0.95, 
                       beta = 0){
  
  # Perform one episode of Q-learning. The agent should move around in the 
  # environment using the given transition model and update the Q-table.
  # The episode ends when the agent reaches a terminal state.
  # 
  # Args:
  #   start_state: array with two entries, describing the starting position of the agent.
  #   epsilon (optional): probability of acting randomly.
  #   alpha (optional): learning rate.
  #   gamma (optional): discount factor.
  #   beta (optional): slipping factor.
  #   reward_map (global variable): a HxW array containing the reward given at each state.
  #   q_table (global variable): a HxWx4 array containing Q-values for each state-action pair.
  # 
  # Returns:
  #   reward: reward received in the episode.
  #   correction: sum of the temporal difference correction terms over the episode.
  #   q_table (global variable): Recall that R passes arguments by value. So, q_table being
  #   a global variable can be modified with the superassigment operator <<-.
  
  
  episode_correction <- 0
  state <- start_state
  repeat{
    #we first get the desired action based on greedy epsilon 
    desired_act <- EpsilonGreedyPolicy(state[1], state[2], epsilon)
    #we (at least attempt) to execute the desired act and see where we land and
    #what reward we get
    new_state <- transition_model(state[1], state[2], desired_act, beta)
    reward <- reward_map[new_state[1], new_state[2]]
    
    score_target <- reward+gamma*max(q_table[new_state[1], new_state[2],])
    curr_q_score <- q_table[state[1], state[2], desired_act]
    diff <- score_target - curr_q_score

    #update the global q_table 
    q_table[state[1], state[2], desired_act] <<- curr_q_score+alpha*diff

    #accumulate corrections
    episode_correction <- episode_correction+abs(diff)
    
    
    state <- new_state
    # Follow policy, execute action, get reward.
    
    # Q-table update.
    
    if(reward!=0)
      # End episode.
      return (c(reward,episode_correction))
  }
  
}
```
